DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/albert-base-v2-config.json HTTP/1.1" 200 0
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-config.json from cache at /root/.cache/torch/transformers/0bbb1531ce82f042a813219ffeed7a1fa1f44cd8f78a652c47fc5311e0d40231.978ff53dd976bbf4bc66f09bf4205da0542be753d025263787842df74d15bbca
INFO:transformers.configuration_utils:Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/albert-base-v2-config.json HTTP/1.1" 200 0
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-config.json from cache at /root/.cache/torch/transformers/0bbb1531ce82f042a813219ffeed7a1fa1f44cd8f78a652c47fc5311e0d40231.978ff53dd976bbf4bc66f09bf4205da0542be753d025263787842df74d15bbca
INFO:transformers.configuration_utils:Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/albert-base-v2-spiece.model HTTP/1.1" 200 0
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-spiece.model from cache at /root/.cache/torch/transformers/dd1588b85b6fdce1320e224d29ad062e97588e17326b9d05a0b29ee84b8f5f93.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/albert-base-v2-config.json HTTP/1.1" 200 0
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-config.json from cache at /root/.cache/torch/transformers/0bbb1531ce82f042a813219ffeed7a1fa1f44cd8f78a652c47fc5311e0d40231.978ff53dd976bbf4bc66f09bf4205da0542be753d025263787842df74d15bbca
INFO:transformers.configuration_utils:Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30000
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443
DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 "HEAD /albert-base-v2-pytorch_model.bin HTTP/1.1" 200 0
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/albert-base-v2-pytorch_model.bin from cache at /root/.cache/torch/transformers/c7c1b2b621933bfa9a5f6ed18b1d6dc2f445001779b13d37286a806117ebeb10.ab806923413c2af99835e13fdbb6014b24af86b0de8edc2d71ef5c646fc54f24
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
using device cuda
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/nlp/datasets/boolq/boolq.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/nlp/datasets/boolq/dataset_infos.json HTTP/1.1" 200 0
INFO:nlp.load:Checking /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py for additional imports.
DEBUG:filelock:Attempting to acquire lock 140590410351896 on /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock
INFO:filelock:Lock 140590410351896 acquired on /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock
INFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq
INFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8
INFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/boolq.py
INFO:nlp.load:Updating dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/dataset_infos.json to /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/dataset_infos.json
INFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/boolq.json
DEBUG:filelock:Attempting to release lock 140590410351896 on /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock
INFO:filelock:Lock 140590410351896 released on /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock
WARNING:nlp.builder:Using custom data configuration default
INFO:nlp.info:Loading Dataset Infos from /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8
INFO:nlp.builder:Overwrite dataset info from restored data version.
INFO:nlp.info:Loading Dataset info from /root/.cache/huggingface/datasets/boolq/default/0.1.0
INFO:nlp.builder:Reusing dataset boolq (/root/.cache/huggingface/datasets/boolq/default/0.1.0)
INFO:nlp.builder:Constructing Dataset for split train, from /root/.cache/huggingface/datasets/boolq/default/0.1.0
--- Logging error ---
Traceback (most recent call last):
  File "/usr/lib/python3.6/logging/__init__.py", line 994, in emit
    msg = self.format(record)
  File "/usr/lib/python3.6/logging/__init__.py", line 840, in format
    return fmt.format(record)
  File "/usr/lib/python3.6/logging/__init__.py", line 577, in format
    record.message = record.getMessage()
  File "/usr/lib/python3.6/logging/__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "main.py", line 36, in <module>
    train_dataset = get_dataset(name="boolq", tokenizer=tokenizer, split='train')
  File "/root/boolq/core.py", line 35, in get_dataset
    logging.info(count,len(answer))
Message: 9427
Arguments: (9427,)
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/nlp/datasets/boolq/boolq.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/nlp/datasets/boolq/dataset_infos.json HTTP/1.1" 200 0
INFO:nlp.load:Checking /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py for additional imports.
DEBUG:filelock:Attempting to acquire lock 140590410352736 on /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock
INFO:filelock:Lock 140590410352736 acquired on /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock
INFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq
INFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8
INFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/boolq.py
INFO:nlp.load:Updating dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/dataset_infos.json to /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/dataset_infos.json
INFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/boolq.json
DEBUG:filelock:Attempting to release lock 140590410352736 on /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock
INFO:filelock:Lock 140590410352736 released on /root/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock
WARNING:nlp.builder:Using custom data configuration default
INFO:nlp.info:Loading Dataset Infos from /usr/local/lib/python3.6/dist-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8
INFO:nlp.builder:Overwrite dataset info from restored data version.
INFO:nlp.info:Loading Dataset info from /root/.cache/huggingface/datasets/boolq/default/0.1.0
INFO:nlp.builder:Reusing dataset boolq (/root/.cache/huggingface/datasets/boolq/default/0.1.0)
INFO:nlp.builder:Constructing Dataset for split validation, from /root/.cache/huggingface/datasets/boolq/default/0.1.0
--- Logging error ---
Traceback (most recent call last):
  File "/usr/lib/python3.6/logging/__init__.py", line 994, in emit
    msg = self.format(record)
  File "/usr/lib/python3.6/logging/__init__.py", line 840, in format
    return fmt.format(record)
  File "/usr/lib/python3.6/logging/__init__.py", line 577, in format
    record.message = record.getMessage()
  File "/usr/lib/python3.6/logging/__init__.py", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "main.py", line 37, in <module>
    test_dataset = get_dataset(name="boolq", tokenizer=tokenizer, split='validation')
  File "/root/boolq/core.py", line 35, in get_dataset
    logging.info(count,len(answer))
Message: 3270
Arguments: (3270,)
  0%|                                                                                                                                                                                              | 0/15 [00:00<?, ?it/s]  0%|                                                                                                                                                                                              | 0/15 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 59, in <module>
    labels = batch_dict[3]
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers/modeling_albert.py", line 893, in forward
    output_hidden_states=output_hidden_states,
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers/modeling_albert.py", line 563, in forward
    output_hidden_states=output_hidden_states,
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers/modeling_albert.py", line 346, in forward
    output_hidden_states,
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers/modeling_albert.py", line 299, in forward
    layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers/modeling_albert.py", line 279, in forward
    ffn_output = self.activation(ffn_output)
  File "/usr/local/lib/python3.6/dist-packages/transformers/activations.py", line 29, in gelu_new
    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.91 GiB total capacity; 10.25 GiB already allocated; 31.00 MiB free; 10.27 GiB reserved in total by PyTorch)
